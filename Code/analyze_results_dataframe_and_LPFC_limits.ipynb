{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze simulation results based on country-wide network\n",
    "* underlying network: entire country = Austria\n",
    "* analyze effects (lost patients, average displacement and free capacity) in single states and on country level\n",
    "* patients steps restricted to max-dist from starting doctor\n",
    "* SIM tries 'max_dist_trials' times to find a doc within reach, then it just chooses another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(iterations,shocks,remov,alpha,max_steps,threshold,kd,max_dist,max_dist_trials,min_pats):\n",
    "    '''   \n",
    "    Set parameters and load datafiles with doc info \n",
    "    * normalize lost patients in states\n",
    "    * calculate all unique patients, unique docs per speciality..\n",
    "    * define parameters\n",
    "    '''\n",
    "    \n",
    "    #list of all specialities 'IM','KI','PSY','ORTR','RAD','DER','URO','HNO','CH','NEU','AU','GGH','AM'\n",
    "    doctors = list(['IM','KI','PSY','ORTR','RAD','DER','URO','HNO','CH','NEU','AU','GGH','AM'])\n",
    "    states = {'state':['Burgenland','Kärnten','Niederösterreich','Oberösterreich','Salzburg','Steiermark',\n",
    "                       'Tirol','Vorarlberg','Wien'],\n",
    "             'state_id':[1,2,3,4,5,6,7,8,9]}\n",
    "    network = 'Österreich' # what is the underlying network for doc connections \n",
    "\n",
    "\n",
    "    # information selection criteria\n",
    "    patient_type = 'total'\n",
    "    capacity_type = 'hour-based'\n",
    "    timeframe = 'quarterly'\n",
    "\n",
    "\n",
    "    ### dataframe with total patient numbers and unique doc numbers per state and per specialty\n",
    "    N = pd.DataFrame.from_dict(states)\n",
    "    N.set_index('state',inplace=True)\n",
    "\n",
    "    for specialization in doctors:\n",
    "        N[specialization+'_total'] = 0\n",
    "        N[specialization+'_cap_total'] = 0\n",
    "        N[specialization+'_unique_docs'] = 0\n",
    "        for bez in N.index:\n",
    "            dinfo = pd.read_csv('data/doctor_info_bez='+network+'_spec='+specialization+'_ptype='+patient_type+\n",
    "                                '_ctype='+capacity_type+'_tf='+timeframe+'_th='+str(threshold)+'.csv',\n",
    "                                usecols=list(['number_of_patients','capacity','gemeinde']))\n",
    "            dinfo.gemeinde = dinfo.gemeinde.astype(str)\n",
    "            N.loc[bez,specialization+'_total'] = dinfo[dinfo.gemeinde.str.startswith(str(N.loc[bez,'state_id']))].number_of_patients.sum()\n",
    "            N.loc[bez,specialization+'_cap_total'] = dinfo[dinfo.gemeinde.str.startswith(str(N.loc[bez,'state_id']))].capacity.sum()\n",
    "            N.loc[bez,specialization+'_unique_docs'] = len(dinfo[dinfo.gemeinde.str.startswith(str(N.loc[bez,'state_id']))])\n",
    "\n",
    "    N.to_excel('results/states_doc_info_{}_{}_{}.xlsx'.format(patient_type, capacity_type, timeframe))\n",
    "\n",
    "    '''\n",
    "    Check results for single states based on entire network\n",
    "    * save all simulation data in one dataframe for seaborn plots\n",
    "    * sum up all lost patients from previous shocks (only possible on country-level, missing info on patients residence)\n",
    "    '''\n",
    "\n",
    "    dta = pd.DataFrame(columns=['run','shock','avg_displacement','N_lost_patients','N_lost_patients_summed',\n",
    "                                'free_capacity_country','free_capacity_state','disconnected_capacity',\n",
    "                                'state','specialty','lost_patients_country','incorrect_displacements',\n",
    "                                'N_lost_patients_state_summed'])\n",
    "\n",
    "    for doc in doctors:  \n",
    "        ### read in the data file\n",
    "        sim_params = 'patient_dynamics_iter{}_shocks{}_remove{}_alpha{}_maxs{}_th{}_kd{}_maxdist{}_maxdisttrials{}_minpats{}_{}_Final_.csv'\\\n",
    "                    .format(iterations, shocks, remov, alpha, max_steps, threshold, kd,max_dist,max_dist_trials,min_pats,doc)\n",
    "        dta_load = pd.read_csv('results/'+sim_params)\n",
    "        for bez in N.state_id:\n",
    "            dta2 = dta_load[dta_load.state==bez].copy()\n",
    "            dta2['state'] = N[N.state_id == bez].index.item()\n",
    "            dta2['specialty'] = doc\n",
    "            dta2['lost_patients_country'] = dta2.N_lost_patients_summed/N.loc[:,doc+'_total'].sum()*100\n",
    "            dta2['lost_patients_state'] = dta2.N_lost_patients_state_summed/N.loc[N.state_id==bez,doc+'_total'].sum()*100\n",
    "            dta2['free_capacity_country'] = dta2.free_capacity_country/N.loc[:,doc+'_cap_total'].sum()*100\n",
    "            dta2['free_capacity_state'] = dta2.free_capacity_state/N.loc[N.state_id==bez,\n",
    "                                                                                 doc+'_cap_total'].item()*100\n",
    "            dta = pd.concat([dta,dta2])\n",
    "\n",
    "\n",
    "    dta.reset_index(drop=True,inplace=True)\n",
    "\n",
    "\n",
    "    dta['num_unique_docs'] = 0\n",
    "    for doc in doctors:\n",
    "        dta.loc[(dta.specialty == doc),'num_unique_docs'] = N.loc[:,doc+'_unique_docs'].sum() \n",
    "\n",
    "    dta['perc_docs_removed'] = dta.shock / dta.num_unique_docs * 100\n",
    "        \n",
    "    \n",
    "    ### calculate remaining free capacity filling up\n",
    "    dta['remaining_FC_filled'] = 0\n",
    "    \n",
    "    for doc in dta.specialty.unique():  \n",
    "        for bez in dta.state.unique():\n",
    "            maxv = dta.loc[(dta.state==bez)&(dta.specialty==doc),'free_capacity_state'].max()\n",
    "            dta.loc[(dta.state==bez)&(dta.specialty==doc),'remaining_FC_filled'] = \\\n",
    "                        dta.loc[(dta.state==bez)&(dta.specialty==doc),'free_capacity_state'].values * (100/maxv)\n",
    "\n",
    "    dta.remaining_FC_filled = 100-dta.remaining_FC_filled\n",
    "    return dta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters of SIM to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100     # number of sim iterations\n",
    "shocks = 5000        # number of maximum doctors to remove\n",
    "remov = 1            # number of docs removed in each step\n",
    "alpha = 0.0          # teleportation probability\n",
    "max_steps = 10       # max steps for patients before becoming lost\n",
    "kd = 1               # keep disconnected doctors 0/1\n",
    "threshold = 0.9      # capacity threshold for calculation\n",
    "max_dist = 100       # maximum travelling distance\n",
    "max_dist_trials = 10 # sim trials to find doc within max distance before using some other doc  \n",
    "min_pats = 2         # minimum number of patients for valid connection in adj.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run function for selected setting\n",
    "res = analyze_results(iterations,shocks,remov,alpha,max_steps,threshold,kd,max_dist,\n",
    "                    max_dist_trials,min_pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results for table in paper\n",
    "* for every state & specialist when limit for LP and FC are reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_copy = res.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_copy.to_csv('results/DF_results_Final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_grouped = res_copy.groupby(['state','specialty','shock']).mean()[['avg_displacement',\n",
    "    'lost_patients_state','remaining_FC_filled','perc_docs_removed','num_unique_docs']].round(2).reset_index()\n",
    "res_grouped.to_csv('results/DF_results_mean_Final.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FC and LP limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### select df up to limit\n",
    "LPlimit = 1\n",
    "FClimit = 20\n",
    "\n",
    "LP = res_grouped[res_grouped.lost_patients_state<=LPlimit]\n",
    "FC = res_grouped[res_grouped.remaining_FC_filled<=FClimit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lost patients per state and specialist for selected limit\n",
    "idx = LP.groupby(['state','specialty'])['lost_patients_state'].transform(max) == LP['lost_patients_state']\n",
    "LP = LP[idx]\n",
    "LP = LP.sort_values(['state','specialty','perc_docs_removed'],ascending=True).drop_duplicates(subset=\n",
    "        ['state','specialty'], keep='last')[['state','specialty','shock','lost_patients_state',\n",
    "                                             'perc_docs_removed']]\n",
    "\n",
    "LP.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### free capacity per state and specialist for selected limit\n",
    "idx = FC.groupby(['state','specialty'])['remaining_FC_filled'].transform(max) == FC['remaining_FC_filled']\n",
    "FC = FC[idx]\n",
    "FC = FC.sort_values(['state','specialty','perc_docs_removed'],ascending=True).drop_duplicates(subset=\n",
    "        ['state','specialty'], keep='last')[['state','specialty','shock','remaining_FC_filled',\n",
    "                                             'perc_docs_removed']]\n",
    "\n",
    "FC.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add info on state ID\n",
    "states = {'state':['Burgenland','Kärnten','Niederösterreich','Oberösterreich',\n",
    "                    'Salzburg','Steiermark','Tirol','Vorarlberg','Wien'],'state_id':[1,2,3,4,5,6,7,8,9]}\n",
    "\n",
    "LP['state_ID'] = 0\n",
    "FC['state_ID'] = 0\n",
    "for s,i in zip(states['state'],states['state_id']):\n",
    "    LP.loc[LP.state==s,'state_ID'] = i\n",
    "    FC.loc[FC.state==s,'state_ID'] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### rename\n",
    "FC.rename(columns={'remaining_FC_filled':'perc_free_capacity','shock':'num_docs_removed'},inplace=True)\n",
    "LP.rename(columns={'lost_patients_state':'perc_lost_patients','shock':'num_docs_removed'},inplace=True)\n",
    "\n",
    "### change dtype\n",
    "LP.num_docs_removed = LP.num_docs_removed.astype(int)\n",
    "FC.num_docs_removed = FC.num_docs_removed.astype(int)\n",
    "\n",
    "### switch order\n",
    "LP = LP[['state', 'state_ID', 'specialty', 'num_docs_removed', 'perc_lost_patients','perc_docs_removed']]\n",
    "FC = FC[['state', 'state_ID', 'specialty', 'num_docs_removed', 'perc_free_capacity','perc_docs_removed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC.to_csv('results/FC_state_specialty_FClimit{}.csv'.format(FClimit),index=False)\n",
    "LP.to_csv('results/LP_state_specialty_LPlimit{}.csv'.format(LPlimit),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC.rename(columns={'num_docs_removed':'fc_num_docs_removed',\n",
    "                   'perc_free_capacity':'fc_perc_free_capacity',\n",
    "                   'perc_docs_removed':'fc_perc_docs_removed'},inplace=True)\n",
    "\n",
    "LP.rename(columns={'num_docs_removed':'lp_num_docs_removed',\n",
    "                   'perc_lost_patients':'lp_perc_lost_patients',\n",
    "                   'perc_docs_removed':'lp_perc_docs_removed'},inplace=True)\n",
    "\n",
    "FC['lp_num_docs_removed'] = LP['lp_num_docs_removed'] \n",
    "FC['lp_perc_lost_patients'] = LP['lp_perc_lost_patients'] \n",
    "FC['lp_perc_docs_removed'] = LP['lp_perc_docs_removed'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FC.to_csv('results/FCLP_limits_{}_{}.csv'.format(FClimit,LPlimit),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
