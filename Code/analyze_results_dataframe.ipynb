{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze simulation results based on country-wide network\n",
    "* underlying network: entire country = Austria\n",
    "* analyze effects (lost patients, average displacement and free capacity) in single states and on country level\n",
    "* patients steps restricted to max-dist from starting doctor\n",
    "* SIM tries 'max_dist_trials' times to find a doc within reach, then it just chooses another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(iterations,shocks,remov,alpha,max_steps,threshold,kd,max_dist,max_dist_trials,min_pats):\n",
    "    '''   \n",
    "    Set parameters and load datafiles with doc info \n",
    "    * normalize lost patients in states\n",
    "    * calculate all unique patients, unique docs per speciality..\n",
    "    * define parameters\n",
    "    '''\n",
    "    \n",
    "    #list of all specialities 'IM','KI','PSY','ORTR','RAD','DER','URO','HNO','CH','NEU','AU','GGH','AM'\n",
    "    doctors = list(['IM','KI','PSY','ORTR','RAD','DER','URO','HNO','CH','NEU','AU','GGH','AM'])\n",
    "    states = {'state':['Burgenland','Kärnten','Niederösterreich','Oberösterreich','Salzburg','Steiermark',\n",
    "                       'Tirol','Vorarlberg','Wien'],\n",
    "             'state_id':[1,2,3,4,5,6,7,8,9]}\n",
    "    network = 'Österreich' # what is the underlying network for doc connections \n",
    "\n",
    "\n",
    "    # information selection criteria\n",
    "    patient_type = 'total'\n",
    "    capacity_type = 'hour-based'\n",
    "    timeframe = 'quarterly'\n",
    "\n",
    "\n",
    "    ### dataframe with total patient numbers and unique doc numbers per state and per specialty\n",
    "    N = pd.DataFrame.from_dict(states)\n",
    "    N.set_index('state',inplace=True)\n",
    "\n",
    "    for specialization in doctors:\n",
    "        N[specialization+'_total'] = 0\n",
    "        N[specialization+'_cap_total'] = 0\n",
    "        N[specialization+'_unique_docs'] = 0\n",
    "        for bez in N.index:\n",
    "            dinfo = pd.read_csv('data/doctor_info_bez='+network+'_spec='+specialization+'_ptype='+patient_type+\n",
    "                                '_ctype='+capacity_type+'_tf='+timeframe+'_th='+str(threshold)+'.csv',\n",
    "                                usecols=list(['number_of_patients','capacity','gemeinde']))\n",
    "            dinfo.gemeinde = dinfo.gemeinde.astype(str)\n",
    "            N.loc[bez,specialization+'_total'] = dinfo[dinfo.gemeinde.str.startswith(str(N.loc[bez,'state_id']))].number_of_patients.sum()\n",
    "            N.loc[bez,specialization+'_cap_total'] = dinfo[dinfo.gemeinde.str.startswith(str(N.loc[bez,'state_id']))].capacity.sum()\n",
    "            N.loc[bez,specialization+'_unique_docs'] = len(dinfo[dinfo.gemeinde.str.startswith(str(N.loc[bez,'state_id']))])\n",
    "\n",
    "    N.to_excel('results/states_doc_info_{}_{}_{}.xlsx'.format(patient_type, capacity_type, timeframe))\n",
    "\n",
    "    '''\n",
    "    Check results for single states based on entire network\n",
    "    * save all simulation data in one dataframe for seaborn plots\n",
    "    * sum up all lost patients from previous shocks (only possible on country-level, missing info on patients residence)\n",
    "    '''\n",
    "\n",
    "    dta = pd.DataFrame(columns=['run','shock','avg_displacement','N_lost_patients','N_lost_patients_summed',\n",
    "                                'free_capacity_country','free_capacity_state','disconnected_capacity',\n",
    "                                'state','specialty','lost_patients_country','incorrect_displacements',\n",
    "                                'N_lost_patients_state_summed'])\n",
    "\n",
    "    ### for all specialists and states, add info on LP and FC\n",
    "    for doc in doctors:  \n",
    "        ### read in the data file\n",
    "        sim_params = 'patient_dynamics_iter{}_shocks{}_remove{}_alpha{}_maxs{}_th{}_kd{}_maxdist{}_maxdisttrials{}_minpats{}_{}_Final_.csv'\\\n",
    "                    .format(iterations, shocks, remov, alpha, max_steps, threshold, kd,max_dist,max_dist_trials,min_pats,doc)\n",
    "        dta_load = pd.read_csv('results/'+sim_params)\n",
    "        \n",
    "        ### change SIM run number (9 states * total_#_docs)\n",
    "        dta_load = dta_load.reset_index(drop=True)\n",
    "        for r in range(0,iterations):\n",
    "            dta_load.loc[r*9*int(dta_load.shock.max()):(r+1)*9*int(dta_load.shock.max()),'run'] = r+1\n",
    "            \n",
    "        for bez in N.state_id:\n",
    "            dta2 = dta_load[dta_load.state==bez].copy()\n",
    "            dta2['state'] = N[N.state_id == bez].index.item()\n",
    "            dta2['specialty'] = doc\n",
    "            dta2['lost_patients_country'] = dta2.N_lost_patients_summed/N.loc[:,doc+'_total'].sum()*100\n",
    "            dta2['lost_patients_state'] = dta2.N_lost_patients_state_summed/N.loc[N.state_id==bez,doc+'_total'].sum()*100\n",
    "            dta2['free_capacity_country'] = dta2.free_capacity_country/N.loc[:,doc+'_cap_total'].sum()*100\n",
    "            dta2['free_capacity_state'] = dta2.free_capacity_state/N.loc[N.state_id==bez,\n",
    "                                                                                 doc+'_cap_total'].item()*100\n",
    "            dta = pd.concat([dta,dta2])\n",
    "\n",
    "\n",
    "    dta.reset_index(drop=True,inplace=True)\n",
    "\n",
    "    ### add number of unique doctors per specialty\n",
    "    dta['num_unique_docs'] = 0\n",
    "    for doc in doctors:\n",
    "        dta.loc[(dta.specialty == doc),'num_unique_docs'] = N.loc[:,doc+'_unique_docs'].sum() \n",
    "\n",
    "    ### the % of removed doctors based on total number\n",
    "    dta['perc_docs_removed'] = dta.shock / dta.num_unique_docs * 100\n",
    "        \n",
    "    \n",
    "    ### calculate remaining free capacity filling up\n",
    "    dta['remaining_FC_filled'] = 0\n",
    "    \n",
    "    for doc in dta.specialty.unique():  \n",
    "        for bez in dta.state.unique():\n",
    "            \n",
    "            if dta.loc[(dta.state==bez)&(dta.specialty==doc),'free_capacity_state'].max() > 0:\n",
    "                maxv = dta.loc[(dta.state==bez)&(dta.specialty==doc),'free_capacity_state'].max()\n",
    "                dta.loc[(dta.state==bez)&(dta.specialty==doc),'remaining_FC_filled'] = \\\n",
    "                            dta.loc[(dta.state==bez)&(dta.specialty==doc),'free_capacity_state'].values * (100/maxv)\n",
    "            else:\n",
    "                dta.loc[(dta.state==bez)&(dta.specialty==doc),'remaining_FC_filled'] = np.nan\n",
    "            \n",
    "                \n",
    "    dta.remaining_FC_filled = 100-dta.remaining_FC_filled\n",
    "    return dta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set parameters of SIM to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100     # number of sim iterations\n",
    "shocks = 5000        # number of maximum doctors to remove\n",
    "remov = 1            # number of docs removed in each step\n",
    "alpha = 0.0            # teleportation probability (try 0 or 0.0 if error - formatting issue)\n",
    "max_steps = 10       # max steps for patients before becoming lost\n",
    "kd = 1               # keep disconnected doctors 0/1\n",
    "threshold = 0.9      # capacity threshold for calculation\n",
    "max_dist = 100       # maximum travelling distance\n",
    "max_dist_trials = 10 # sim trials to find doc within max distance before using some other doc  \n",
    "min_pats = 2         # minimum number of patients for valid connection in adj.matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### run function for selected setting\n",
    "res = analyze_results(iterations,shocks,remov,alpha,max_steps,threshold,kd,max_dist,\n",
    "                    max_dist_trials,min_pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv('results/DF_results_Final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
